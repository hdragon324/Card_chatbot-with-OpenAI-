from utils.recommend import recommend_my_model_test
import json

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from openai import OpenAI

# ------------------- ì‚¬ì „ ì„¤ì • ë¶€ë¶„ -------------------
chat_model = ChatOpenAI(model_name='gpt-3.5-turbo',
                        api_key=st.secrets['OPENAI_API_KEY'],
                        temperature=0.5)

my_prompt = PromptTemplate(input_variables=['chat_history', 'question'],
                        template='''You are an AI assistant. You are currently having a converasation with a human. Answer the Question.
                        chat_history : {chat_history},
                        Human : {question},
                        AI_assistant:''')

# ì¼ë°˜ì ì¸ ì½”ë“œì—ì„œëŠ” memoryê°œì²´ë¥¼ ìƒì„±í•˜ë©´ ëŒ€í™” ë‚´ìš©ë“¤ì„ ìë™ìœ¼ë¡œ ê¸°ì–µí•˜ì§€ë§Œ streamlitì—ì„œëŠ” ì›¹ì—ì„œ ìš”ì²­, ì‘ë‹µì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—
# ë”°ë¡œ ì €ì¥í•´ë‘ì§€ ì•Šìœ¼ë©´ ë‹¤ ì´ˆê¸°í™” ë¨

# session_stateì— ì²˜ìŒë¶€í„° ê°’ì„ ì €ì¥í•˜ì§€ ì•Šê³  if not inì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì½”ë“œë¥¼ ì¬ ì‹¤í–‰ ì‹œì¼°ì„ ê²½ìš° ê¸°ì¡´ session_stateì— ì €ì¥ëœ ê°’ì´
# ì´ˆê¸°í™” ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
if 'pre_memory' not in st.session_state:
    st.session_state.pre_memory = ConversationBufferMemory(memory_key='chat_history',
                                                           return_messages=True)
    
llm_chain = LLMChain(llm=chat_model,
                     prompt=my_prompt,
                     # .pre_memoryë¡œ ì ‘ê·¼í•˜ë©´ ì„¸ì…˜ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” CBMemoryê°ì²´ê°€ ì…ë ¥ë¨
                     memory=st.session_state.pre_memory)


# LLMì— ìš”ì²­í•˜ê³  ì‘ë‹µ ë°›ì„ ìˆ˜ ìˆëŠ” chat í˜•íƒœë¡œ messagesë¥¼ ê´€ë¦¬
if 'messages' not in st.session_state:
    st.session_state.messages = [{'role':'assistant','content':'ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì†Œë¹„ìì˜ ì†Œë¹„ ìŠµê´€ì— ë§ì¶° ìµœì ì˜ ì¹´ë“œë¥¼ ì¶”ì²œí•´ì£¼ëŠ” CARA (Card Advisor & Recommendation AI) ì…ë‹ˆë‹¤.'}]

# ------------------- ----------------- --------------
# ------------------- ì›¹ í‘œì‹œ ë¶€ë¶„ --------------------
# CSS ìŠ¤íƒ€ì¼ ì •ì˜
st.markdown(
    """
    <style>
    .title {
        font-size: 70px;
        font-weight: bold;
        color: #003366; /* ì§„í•œ ë‚¨ìƒ‰ */
        text-align: center;
        padding: 15px;
        border: 4px solid #003366; /* ë‘êº¼ìš´ í…Œë‘ë¦¬ */
        border-radius: 12px;
        background: linear-gradient(135deg, #4A90E2, #A0C4FF); /* íŒŒë€ìƒ‰ ê³„ì—´ ê·¸ë¼ë””ì–¸íŠ¸ */
        box-shadow: 4px 4px 15px rgba(0, 0, 0, 0.3); /* ë” ê°•í•œ ê·¸ë¦¼ì */
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ì œëª© í‘œì‹œ
st.markdown('<p class="title">ğŸ’³ ì¹´ë“œ ì¶”ì²œ ì±—ë´‡ : CARA ğŸ’³</p>', unsafe_allow_html=True)


# ë°˜ë³µë¬¸ìœ¼ë¡œ session_state.messagesì— ìˆëŠ” ëª¨ë“  ëŒ€í™” ê¸°ë¡ì— ì ‘ê·¼
for message in st.session_state.messages : 
    # chat_message : ë©”ì‹œì§€ì˜ ë°œì‹ ì roleì— ë”°ë¼ UIë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥
    #               assistant, userëŠ” ë””í´íŠ¸ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©° ë‹¤ë¥¸ roleì„ ì¶”ê°€í•˜ë ¤ë©´ HTMLì½”ë“œ ì‘ì„±ì´ í•„ìš”
    with st.chat_message(message['role']) :
        st.write(message['content'])

# chat_input : ì±„íŒ… ë©”ì‹œì§€ ì…ë ¥ UIê°€ ìƒì„±ë˜ë©° ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
user_prompt = st.chat_input()

if user_prompt is not None:
    st.session_state.messages.append({'role':'user','content':user_prompt})
    with st.chat_message('user'): # user í˜•íƒœë¡œ ë©”ì‹œì§€ ì°½ UI ì¶œë ¥
        st.write(user_prompt)     # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ ì¶œë ¥


# ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ roleì´ assistantê°€ ì•„ë‹ˆë¼ë©´ ìƒˆë¡œìš´ ë‹µë³€ì„ ìƒì„±í•˜ê³  session_stateì— ì¶”ê°€
# (assistantê°€ ë³¸ì¸ì˜ ì‘ë‹µì—ë„ ê³„ì† ëŒ€ë‹µí•˜ëŠ” ìë¬¸ìë‹µì„ ë°©ì§€í•˜ê¸° ìœ„í•´)
if st.session_state.messages[-1]['role'] != 'assistant' :
    # assistant ë©”ì‹œì§€ UI ìƒì„±
    with st.chat_message('assistant') :
        # AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ì´ ì‹¤í–‰ë¨
        with st.spinner('Loading...') :
            try:
                # ì‚¬ìš©ìê°€ ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œê³¼ ê´€ë ¨ëœ ì´ì•¼ê¸°ë¥¼ í–ˆë‹¤ë©´
                if any(keyword in user_prompt.lower() for keyword in ["ì¹´ë“œì¶”ì²œ", "ì¹´ë“œ ì¶”ì²œ", "ì¶”ì²œ"]):

                    # ì‘ë‹µê³¼ ì¹´ë“œ ì½”ë“œë¥¼ ë°˜í™˜
                    ai_response, card_code = recommend_my_model_test(user_prompt)

                    # ì¹´ë“œ ì´ë¯¸ì§€ë¥¼ ai_response ìƒë‹¨ì— ì¶œë ¥í•¨
                    image_path = f"card_image/{card_code}card.png"
                    st.image(image_path, width=300)
                    st.write(f'- **ì¹´ë“œê³ ë¦´ë¼ URL** : https://www.card-gorilla.com/card/detail/{card_code}')
                
                # ì‚¬ìš©ìê°€ ì±„íŒ… ë©”ì‹œì§€ë¡œ ì—­í• ì— ê´€í•´ ë¬¼ì–´ë´¤ë‹¤ë©´
                elif any(keyword in user_prompt.lower() for keyword in ["ì—­í• ", "role"]):
                    ai_response = 'ì €ì˜ ì—­í• ì€ ì†Œë¹„ì ìŠµê´€ì— ë§ì¶° ì¹´ë“œë¥¼ ì¶”ì²œí•˜ëŠ” ë´‡ì…ë‹ˆë‹¤.'
                else:
                    ai_response = llm_chain.predict(question=user_prompt)
                st.write(ai_response)
                # ëª¨ë¸ì˜ ì‘ë‹µë„ ì„¸ì…˜ì˜ messagesì— ì¶”ê°€í•˜ì—¬ ê´€ë¦¬
                st.session_state.messages.append({'role':'assistant','content':ai_response})
            except Exception as e:
                st.error(f'LLM ì—ëŸ¬ ë°œìƒ : {e}')